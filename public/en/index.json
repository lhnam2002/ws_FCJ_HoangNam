[
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Context and Motivation In today‚Äôs data-driven world, the ability to process and analyze data in an efficient, automated, and scalable manner is essential to supporting modern business processes and decision-making.\nThe project ‚ÄúBuilding a Serverless Data Processing Pipeline with AWS Step Functions and Amazon EventBridge‚Äù introduces a serverless data processing solution, leveraging the robust capabilities of Amazon Web Services (AWS) to meet the needs for high performance, flexibility, and cost-efficiency.\nThe pipeline is designed to automatically process CSV files uploaded to Amazon S3. Amazon EventBridge detects the file upload event and triggers a workflow orchestrated by AWS Step Functions, which includes validation, parallel data processing, result aggregation, and final storage.\nProcessing tasks are executed via AWS Lambda functions. Metadata is stored in Amazon DynamoDB, status notifications are sent through Amazon SNS, and the entire system is monitored via Amazon CloudWatch for performance tracking and error detection.\nKey Benefits of Serverless Data Processing with Step Functions and EventBridge By utilizing core AWS services such as Amazon S3, EventBridge, Step Functions, Lambda, DynamoDB, CloudWatch, and SNS, this solution offers several key advantages:\nFully automated workflows:\nEvent-driven triggers initiate and orchestrate the entire pipeline seamlessly.\nElastic scalability:\nAutomatically scales with incoming workload, supporting parallel data processing.\nCost efficiency:\nPay-as-you-go pricing eliminates infrastructure overhead.\nRobust error handling:\nAutomatic retries, error notifications via SNS, and observability through CloudWatch.\nFlexibility and maintainability:\nModular design using Lambda functions allows for easy updates and debugging.\nPowerful monitoring:\nCloudWatch provides detailed logs and real-time alerts.\nInput data validation:\nEnsures only valid and clean data is processed.\nSimplified operations:\nEasy management of resource lifecycle ‚Äî create, update, and tear down.\nReal-world applicability:\nIdeal for ETL pipelines, batch processing, and real-time event-driven data analytics.\n"
},
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/",
	"title": "Serverless Data Processing Pipeline",
	"tags": [],
	"description": "",
	"content": "Building a Serverless Data Processing Pipeline with AWS Step Functions and EventBridge üìå Overview In today‚Äôs technological landscape, the demand for efficient, flexible, and cost-effective data processing is becoming increasingly critical. Organizations require scalable and easily manageable solutions that eliminate the burden of maintaining complex infrastructure.\nServerless technology on the cloud has emerged as an optimal approach, allowing the development of powerful data processing systems without the need to manage physical servers. This reduces operational costs, accelerates deployment, and enables high automation.\nüß© Pipeline Overview This serverless pipeline processes data from CSV files uploaded to Amazon S3. Amazon EventBridge listens to events from S3, filters for .csv files, and triggers AWS Step Functions to orchestrate the processing workflow. AWS Lambda handles tasks such as validation, parallel processing, aggregation, and result storage. Amazon DynamoDB stores metadata, Amazon CloudWatch monitors performance and errors, and Amazon SNS sends email notifications about the pipeline status.\nThe entire solution is deployed in the Singapore region (ap-southeast-1).\nKey Components Amazon S3\nInput Bucket: Stores the input CSV files\ndata-processing-input-\u0026lt;your-account-id\u0026gt; Output Bucket: Stores processed JSON results\ndata-processing-output-\u0026lt;your-account-id\u0026gt; Amazon EventBridge\nFilters ObjectCreated events from S3 for .csv files and triggers Step Functions AWS Step Functions (State Machine: DataProcessingWorkflow)\nValidateData: Validates the uploaded CSV file ParallelProcess: Executes two parallel branches (ProcessData1, ProcessData2) AggregateData: Aggregates results from both parallel branches StoreResults: Stores the final results to S3 and metadata to DynamoDB NotifySuccess: Sends a success notification via SNS ErrorHandler: Sends an error notification via SNS AWS Lambda\nValidateDataFunction: Validates the CSV format ProcessDataFunction: Processes data (used in each parallel branch) AggregateDataFunction: Aggregates intermediate results StoreResultsFunction: Saves final results and metadata Amazon DynamoDB\nProcessingMetadata table stores information such as ExecutionId, Timestamp, and Status Amazon CloudWatch\nLogs events and sets alarms for failures or anomalies Amazon SNS\nPipelineNotifications topic sends email alerts about pipeline status IAM Roles\nLambdaDataProcessingRole: Grants Lambda permission to access S3, DynamoDB, and SNS StepFunctionsDataProcessingRole: Grants Step Functions permission to invoke Lambda and SNS üîÅ Data Flow A user uploads a CSV file (e.g., test.csv) to the S3 Input Bucket\nS3 triggers an ObjectCreated event, which is sent to EventBridge\nEventBridge Rule (S3TriggerRule) filters .csv files and triggers the Step Functions workflow (DataProcessingWorkflow)\nStep Functions orchestrates the following steps:\nValidateData: Calls ValidateDataFunction to validate the CSV file ParallelProcess: Executes two parallel branches, each invoking ProcessDataFunction AggregateData: Calls AggregateDataFunction to merge results StoreResults: Invokes StoreResultsFunction to save outputs to S3 and metadata to DynamoDB NotifySuccess: Sends a success notification via SNS ErrorHandler: Sends an error notification via SNS if any step fails CloudWatch logs execution and triggers alerts in case of errors\nSNS sends email notifications about the final pipeline status (success or failure)\n"
},
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/2-preparation/",
	"title": "Preparation",
	"tags": [],
	"description": "",
	"content": "Preparation Steps 1. Create IAM Components Step 1: Create an IAM User Group Go to AWS Console: https://console.aws.amazon.com/iam Select User groups ‚Üí Create group Group name: devGr Attach permissions: Choose the existing policy: AdministratorAccess Click Create group Step 2: Create an IAM User Go to Users ‚Üí click Add users Enter user name: dev-user Choose Access Types: Programmatic access (for AWS CLI usage) AWS Management Console access (for web console login) Set a password or let AWS auto-generate one Add the user to group: devGr Click Create user Step 3: Create an AWS Access Key In the user list, click on dev-user Navigate to the Security credentials tab Under Access keys, click Create access key Choose usage purpose: Command Line Interface (CLI) Once created, you will receive: Access key ID Secret access key ‚ö†Ô∏è Save it immediately, the Secret access key is shown only once 2. Install and Configure AWS CLI üîπ Install AWS CLI on Windows Open the Run dialog (Windows + R), paste the following command: msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi Verify the CLI Version aws --version Configure AWS CLI aws configure Provide the following information when prompted:\n\u0026ndash; Access Key ID\n\u0026ndash; Secret Access Key\n\u0026ndash; Region (ap-southeast-1)\n\u0026ndash; Output format (json)\n3. Create a Sample CSV File Create a CSV File Using Notepad Press Windows + R, type notepad, and hit Enter\nPaste the following content:\nid,name,amount 1,John,100 2,Jane,200 Go to File \u0026gt; Save As\nFile name: test.csv\nSave as type: All Files (.)\nEncoding: UTF-8\n‚úÖ Make sure the file is not saved as test.csv.txt\n"
},
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/3-creates3/",
	"title": "Create S3 Buckets",
	"tags": [],
	"description": "",
	"content": "‚òÅÔ∏è Step 1: Create S3 Buckets Amazon S3 is used to store input and output data for the serverless data processing pipeline.\n1. Access the S3 Console Go to AWS Console: https://console.aws.amazon.com/s3 In the search bar, type S3 and select Amazon S3 2. Create Input Bucket Click Create bucket\nConfigure the following:\nBucket name: data-processing-input-123456789012\n(replace 123456789012 with your actual AWS Account ID)\nRegion: Select Asia Pacific (Singapore) - ap-southeast-1\nObject Ownership: Choose ACLs disabled\nBlock Public Access: Leave default (block all public access)\nEncryption: Enable\nServer-side encryption with Amazon S3-managed keys (SSE-S3)\nClick Create bucket\n‚ö†Ô∏è If you receive the error \u0026ldquo;Bucket name already exists\u0026rdquo;, add a suffix to make it unique, e.g.:\ndata-processing-input-123456789012-v1\n3. Create Output Bucket Repeat the same steps as above Use the name: data-processing-output-123456789012 4. Verify the Buckets Go to the Buckets list in the S3 console\nConfirm both buckets are created:\ndata-processing-input-123456789012 data-processing-output-123456789012 Optional: Upload test.csv to Input Bucket Open the data-processing-input-123456789012 bucket Click Upload Select the previously created test.csv file Click Upload to confirm "
},
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/4-create-iam-roles/",
	"title": "T·∫°o IAM Roles",
	"tags": [],
	"description": "",
	"content": "Step 2: Create IAM Roles IAM Roles grant permissions to AWS services such as Lambda and Step Functions to access other AWS resources required during the data processing workflow.\n1. Access the IAM Console Go to AWS Console: https://console.aws.amazon.com/iam Search for IAM ‚Üí go to Roles ‚Üí click Create role 2. Create Role for Lambda Trusted entity type: Select AWS service ‚Üí choose Lambda Click Next Attach Permissions: Click Add permissions\nSearch and attach the following policies:\nAWSLambdaBasicExecutionRole (for writing logs to CloudWatch) AmazonS3FullAccess (to access S3 buckets) AmazonDynamoDBFullAccess (to access DynamoDB) AmazonSNSFullAccess (to send notifications via SNS) üîí Security Note: For production environments, consider creating custom policies with fine-grained access to only the necessary resources.\nClick Next Role name: LambdaDataProcessingRole Click Create role 3. Create Role for Step Functions Return to IAM \u0026gt; Roles ‚Üí click Create role Trusted entity type: Select AWS service ‚Üí choose Step Functions Click Next Attach Permissions: Attach the following policies:\nAWSLambdaFullAccess (allows Step Functions to invoke Lambda functions) AmazonS3FullAccess AmazonDynamoDBFullAccess CloudWatchLogsFullAccess AmazonSNSFullAccess Click Next\nRole name: StepFunctionsDataProcessingRole\nClick Create role\n4. Verify Roles Go to IAM \u0026gt; Roles\nConfirm that both roles are created:\nLambdaDataProcessingRole StepFunctionsDataProcessingRole "
},
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/5-createlambda/",
	"title": "Create Lambda Funtion",
	"tags": [],
	"description": "",
	"content": "Step 3: Create Lambda Functions Create 4 Lambda functions to handle each step of the serverless data processing pipeline.\n1. Access the Lambda Console Open AWS Console: https://console.aws.amazon.com/lambda Search for Lambda, go to Functions, and click Create function 2. Create ValidateDataFunction Function name: ValidateDataFunction Runtime: Python 3.9 (or newer) Architecture: x86_64 Permissions: Choose Use an existing role ‚Üí select LambdaDataProcessingRole Click Create function ‚úÖ Code (Tab: Code \u0026gt; Code source) import json import boto3 import csv import io def lambda_handler(event, context): s3 = boto3.client(\u0026#39;s3\u0026#39;) try: bucket = event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] response = s3.get_object(Bucket=bucket, Key=key) data = response[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;) csv_reader = csv.reader(io.StringIO(data)) headers = next(csv_reader, None) if not headers or len(headers) \u0026lt; 2: raise Exception(\u0026#34;Invalid CSV: Missing or insufficient headers\u0026#34;) first_row = next(csv_reader, None) if not first_row: raise Exception(\u0026#34;Invalid CSV: No data rows\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;bucket\u0026#39;: bucket, \u0026#39;key\u0026#39;: key, \u0026#39;valid\u0026#39;: True, \u0026#39;headers\u0026#39;: headers }) } except Exception as e: raise Exception(f\u0026#34;Validation failed: {str(e)}\u0026#34;) Click Deploy Configuration Timeout: 30 seconds Memory: 256 MB Environment variable: INPUT_BUCKET = data-processing-input-123456789012 3. Create ProcessDataFunction Repeat creation, name the function: ProcessDataFunction import json def lambda_handler(event, context): try: data = json.loads(event[\u0026#39;body\u0026#39;]) bucket = data[\u0026#39;bucket\u0026#39;] key = data[\u0026#39;key\u0026#39;] processed_data = { \u0026#39;result\u0026#39;: f\u0026#34;Processed file {key} from {bucket}\u0026#34;, \u0026#39;transformed\u0026#39;: True } return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(processed_data) } except Exception as e: raise Exception(f\u0026#34;Processing failed: {str(e)}\u0026#34;) Click Deploy Configuration Timeout: 30 seconds Memory: 256 MB 4. Create AggregateDataFunction Repeat creation, name the function: AggregateDataFunction import json def lambda_handler(event, context): try: results = [json.loads(item[\u0026#39;body\u0026#39;]) for item in event] aggregated = { \u0026#39;aggregated_result\u0026#39;: [r[\u0026#39;result\u0026#39;] for r in results], \u0026#39;total_branches\u0026#39;: len(results) } return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(aggregated) } except Exception as e: raise Exception(f\u0026#34;Aggregation failed: {str(e)}\u0026#34;) Click Deploy Configuration Timeout: 30 seconds Memory: 256 MB 5. Create StoreResultsFunction Repeat creation, name the function: StoreResultsFunction import json import boto3 import datetime def lambda_handler(event, context): s3 = boto3.client(\u0026#39;s3\u0026#39;) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(\u0026#39;ProcessingMetadata\u0026#39;) try: data = json.loads(event[\u0026#39;body\u0026#39;]) output_bucket = \u0026#39;data-processing-output-123456789012\u0026#39; # Replace with your actual bucket output_key = f\u0026#34;results/processed_{datetime.datetime.now().isoformat()}.json\u0026#34; s3.put_object(Bucket=output_bucket, Key=output_key, Body=json.dumps(data)) table.put_item(Item={ \u0026#39;ExecutionId\u0026#39;: context.aws_request_id, \u0026#39;Timestamp\u0026#39;: datetime.datetime.now().isoformat(), \u0026#39;OutputKey\u0026#39;: output_key, \u0026#39;Status\u0026#39;: \u0026#39;Success\u0026#39; }) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;status\u0026#39;: \u0026#39;Stored successfully\u0026#39;, \u0026#39;output_key\u0026#39;: output_key}) } except Exception as e: raise Exception(f\u0026#34;Storage failed: {str(e)}\u0026#34;) Replace data-processing-output-123456789012 with your actual S3 bucket name Configuration Timeout: 1 minute Memory: 512 MB 6. Test Each Lambda Function In each Lambda function ‚Üí go to the Test tab ‚Üí click Create test event\nUse the following test payload:\n{ \u0026#34;Records\u0026#34;: [ { \u0026#34;s3\u0026#34;: { \u0026#34;bucket\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;data-processing-input-123456789012\u0026#34;}, \u0026#34;object\u0026#34;: {\u0026#34;key\u0026#34;: \u0026#34;test.csv\u0026#34;} } } ] } Click Test\nCheck logs in Monitor \u0026gt; Logs\n"
},
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/6-createdynamodbtable/",
	"title": "Create DynamoDB Table",
	"tags": [],
	"description": "",
	"content": "Step 4: Create DynamoDB Table Amazon DynamoDB is used to store metadata for each pipeline execution.\n1. Access DynamoDB Console Go to AWS Console Search for DynamoDB ‚Üí select Tables ‚Üí click Create table 2. Configure the Table Table name: ProcessingMetadata Partition key: ExecutionId (Type: String) Sort key: (leave empty) Table settings: Select On-demand (Pay-per-request) for automatic scaling Click Create table 3. Verify the Table Go to the Tables section Ensure the ProcessingMetadata table has been created successfully "
},
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/7-createsnstopic/",
	"title": "Create SNS Topic",
	"tags": [],
	"description": "",
	"content": "üì¨ Step 5: Create SNS Topic Amazon SNS will be used to send pipeline status notifications via email.\n1. Access SNS Console Go to AWS Console Search for SNS Choose Topics ‚Üí click Create topic 2. Configure the Topic Type: Standard Name: PipelineNotifications Click Create topic 3. Create Subscription Inside the PipelineNotifications topic, click Create subscription Protocol: Email Endpoint: Enter your email address Click Create subscription Check your email and confirm the subscription via the link provided 4. Get the Topic ARN Go back to the Topics page Click on PipelineNotifications Copy the Topic ARN, e.g.: (arn:aws:sns:ap-southeast-1:123456789012:PipelineNotifications) "
},
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/8-createstepfunctions/",
	"title": "Create Step Functions State Machine",
	"tags": [],
	"description": "",
	"content": "Step 6: Create Step Functions State Machine AWS Step Functions will orchestrate the entire serverless data processing pipeline.\n1. Access the Step Functions Console Open AWS Console Search for Step Functions Select State machines ‚Üí click Create state machine 2. Choose the State Machine Type Author with code Type: Standard 3. Define the State Machine Paste the JSON definition for the state machine: { \u0026#34;Comment\u0026#34;: \u0026#34;Serverless Data Processing Pipeline\u0026#34;, \u0026#34;StartAt\u0026#34;: \u0026#34;ValidateData\u0026#34;, \u0026#34;States\u0026#34;: { \u0026#34;ValidateData\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:123456789012:function:ValidateDataFunction\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;ParallelProcess\u0026#34;, \u0026#34;Retry\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;States.ALL\u0026#34; ], \u0026#34;IntervalSeconds\u0026#34;: 3, \u0026#34;MaxAttempts\u0026#34;: 3, \u0026#34;BackoffRate\u0026#34;: 2 } ], \u0026#34;Catch\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;States.ALL\u0026#34; ], \u0026#34;ResultPath\u0026#34;: \u0026#34;$.error\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;ErrorHandler\u0026#34; } ] }, \u0026#34;ParallelProcess\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Parallel\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;AggregateData\u0026#34;, \u0026#34;Branches\u0026#34;: [ { \u0026#34;StartAt\u0026#34;: \u0026#34;ProcessData1\u0026#34;, \u0026#34;States\u0026#34;: { \u0026#34;ProcessData1\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:123456789012:function:ProcessDataFunction\u0026#34;, \u0026#34;End\u0026#34;: true } } }, { \u0026#34;StartAt\u0026#34;: \u0026#34;ProcessData2\u0026#34;, \u0026#34;States\u0026#34;: { \u0026#34;ProcessData2\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:123456789012:function:ProcessDataFunction\u0026#34;, \u0026#34;End\u0026#34;: true } } } ], \u0026#34;Catch\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;States.ALL\u0026#34; ], \u0026#34;ResultPath\u0026#34;: \u0026#34;$.error\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;ErrorHandler\u0026#34; } ] }, \u0026#34;AggregateData\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:123456789012:function:AggregateDataFunction\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;StoreResults\u0026#34;, \u0026#34;Retry\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;States.ALL\u0026#34; ], \u0026#34;IntervalSeconds\u0026#34;: 3, \u0026#34;MaxAttempts\u0026#34;: 3, \u0026#34;BackoffRate\u0026#34;: 2 } ], \u0026#34;Catch\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;States.ALL\u0026#34; ], \u0026#34;ResultPath\u0026#34;: \u0026#34;$.error\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;ErrorHandler\u0026#34; } ] }, \u0026#34;StoreResults\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:123456789012:function:StoreResultsFunction\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;NotifySuccess\u0026#34;, \u0026#34;Catch\u0026#34;: [ { \u0026#34;ErrorEquals\u0026#34;: [ \u0026#34;States.ALL\u0026#34; ], \u0026#34;ResultPath\u0026#34;: \u0026#34;$.error\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;ErrorHandler\u0026#34; } ] }, \u0026#34;NotifySuccess\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::sns:publish\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;Message\u0026#34;: \u0026#34;Pipeline completed successfully\u0026#34;, \u0026#34;Subject\u0026#34;: \u0026#34;Pipeline Status\u0026#34;, \u0026#34;TopicArn\u0026#34;: \u0026#34;arn:aws:sns:ap-southeast-1:123456789012:PipelineNotifications\u0026#34; }, \u0026#34;End\u0026#34;: true }, \u0026#34;ErrorHandler\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::sns:publish\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;Message.$\u0026#34;: \u0026#34;States.JsonToString($.error)\u0026#34;, \u0026#34;Subject\u0026#34;: \u0026#34;Pipeline Error\u0026#34;, \u0026#34;TopicArn\u0026#34;: \u0026#34;arn:aws:sns:ap-southeast-1:123456789012:PipelineNotifications\u0026#34; }, \u0026#34;End\u0026#34;: true } } } Replace the following:\n123456789012 ‚Üí with your AWS Account ID Lambda function ARNs ‚Üí copy from Lambda Console SNS topic ARN ‚Üí copy from SNS Console Click Next\n4. Configure the State Machine Name: DataProcessingWorkflow\nPermissions:\nChoose Choose an existing role Select role: StepFunctionsDataProcessingRole Logging:\nEnable Log to CloudWatch Logs Set level: ALL Click Create state machine\n5. Verify Go back to State machines list Ensure that DataProcessingWorkflow appears successfully "
},
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/9-createeventbridge/",
	"title": "Create EventBridge Rule",
	"tags": [],
	"description": "",
	"content": "Step 7: Create EventBridge Rule Amazon EventBridge will listen for S3 events and trigger the Step Functions workflow when a new file is uploaded.\n1. Access the EventBridge Console Go to AWS Console Search for EventBridge Select Rules ‚Üí click Create rule 2. Configure the Rule Name: S3TriggerRule Event source: select Event Pattern Event pattern (JSON): { \u0026#34;source\u0026#34;: [\u0026#34;aws.s3\u0026#34;], \u0026#34;detail-type\u0026#34;: [\u0026#34;Object Created\u0026#34;], \u0026#34;detail\u0026#34;: { \u0026#34;bucket\u0026#34;: { \u0026#34;name\u0026#34;: [\u0026#34;data-processing-input-123456789012\u0026#34;] }, \u0026#34;object\u0026#34;: { \u0026#34;key\u0026#34;: [{\u0026#34;suffix\u0026#34;: \u0026#34;.csv\u0026#34;}] } } } Note: Replace data-processing-input-123456789012 with your actual S3 bucket name. 3. Set Target Target type: Select Step Functions state machine State machine: choose DataProcessingWorkflow Execution role: select Create a new role for this specific resource -Click Create 4. Verify the Rule Return to the Rules section Ensure the S3TriggerRule has been created and is Enabled "
},
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/10-configurecloudwatch/",
	"title": "Configure CloudWatch",
	"tags": [],
	"description": "",
	"content": "Step 8: Configure CloudWatch Monitoring Amazon CloudWatch helps monitor performance and detect failures in your data pipeline.\n1. Access the CloudWatch Console Open AWS Console Search for CloudWatch and open the service 2. Create Alarm for Step Functions Go to Alarms ‚Üí Click Create alarm Select metric: Choose: States ‚Üí Execution Metrics ‚Üí ExecutionsFailed Select: DataProcessingWorkflow Conditions: Threshold: Greater than or equal to 1 Period: 5 minutes Actions: Select: Send notification to SNS topic SNS Topic: PipelineNotifications Alarm name: StepFunctionsFailureAlarm Click Create 3. Create Alarms for Lambda Functions Repeat the process for each Lambda function using the Errors metric.\nLambda Functions: ValidateDataFunction ProcessDataFunction AggregateDataFunction StoreResultsFunction Steps: Go to Alarms ‚Üí Click Create alarm Select metric: Navigate to: Lambda ‚Üí By Function Name Choose the function ‚Üí Select Errors metric Conditions: Threshold type: Static Condition: Greater than or equal to 1 Period: 5 minutes Datapoints to alarm: 1 out of 1 Notifications: Alarm state trigger: In alarm Send notification to: PipelineNotifications SNS topic Alarm name (clear naming): LambdaValidateDataErrorAlarm LambdaProcessDataErrorAlarm LambdaAggregateDataErrorAlarm LambdaStoreResultsErrorAlarm Click Create 4. Check Logs Go to Log groups Check the following log groups: Lambda: /aws/lambda/ValidateDataFunction /aws/lambda/ProcessDataFunction /aws/lambda/AggregateDataFunction /aws/lambda/StoreResultsFunction Step Functions: /aws/vendedlogs/states/DataProcessingWorkflow "
},
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/11-testthepipeline/",
	"title": "Test the Pipeline",
	"tags": [],
	"description": "",
	"content": "Step 9: Test the Pipeline Perform a full test of the serverless data pipeline after deployment.\n1. Upload CSV File to S3 Go to the S3 Console Navigate to the bucket: data-processing-input-123456789012 Click Upload, select the file test.csv Click Upload 2. Monitor Step Functions Execution Open the Step Functions Console Select the state machine: DataProcessingWorkflow Go to the Executions tab Click on the latest execution to inspect the flow Review individual steps: in the Visual workflow. ValidateData ParallelProcess AggregateData StoreResults NotifySuccess 3. Check Output Go to the output S3 bucket: data-processing-output-123456789012 Locate the folder results/ and verify the result file (e.g., processed_\u0026lt;timestamp\u0026gt;.json) Open the DynamoDB Console Navigate to table ProcessingMetadata Use Explore table items to inspect execution metadata 4. Verify SNS Notifications Check your email inbox Look for the notification from the PipelineNotifications SNS topic ‚ùó 5. Error Handling (if any) Open the CloudWatch Console Go to Log groups ‚Üí check logs for relevant Lambda functions or Step Functions Revisit Step Functions ‚Üí Executions to review failure state Identify the failed step ‚Üí update Lambda code or state machine definition as needed "
},
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/12--optimizationandcostanalysis/",
	"title": "Optimization and Cost Analysis",
	"tags": [],
	"description": "",
	"content": "Step 10: Optimization and Cost Analysis 1. Parallel Optimization In the ParallelProcess state of Step Functions, there are currently two branches. To add a new branch:\nGo to Step Functions Console, select DataProcessingWorkflow, and click Edit. Add a new branch inside the Branches section of ParallelProcess. { \u0026#34;StartAt\u0026#34;: \u0026#34;ProcessData3\u0026#34;, \u0026#34;States\u0026#34;: { \u0026#34;ProcessData3\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:123456789012:function:ProcessDataFunction\u0026#34;, \u0026#34;End\u0026#34;: true } } } Click Save to apply the changes. 2. Error Handling Retry logic: Each step (ValidateData, AggregateData, StoreResults) retries up to 3 times with a 3-second delay and a backoff rate of 2.0. ErrorHandler: If a step fails, the state transitions to ErrorHandler, which sends a notification via SNS. Advanced option: You can optionally add a new state to log the error into DynamoDB for traceability. \u0026#34;ErrorHandler\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:123456789012:function:StoreResultsFunction\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;error.$\u0026#34;: \u0026#34;$.error\u0026#34; }, \u0026#34;Next\u0026#34;: \u0026#34;NotifyError\u0026#34; }, \u0026#34;NotifyError\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::sns:publish\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;Message.$\u0026#34;: \u0026#34;States.JsonToString($.error)\u0026#34;, \u0026#34;Subject\u0026#34;: \u0026#34;Pipeline Error\u0026#34;, \u0026#34;TopicArn\u0026#34;: \u0026#34;arn:aws:sns:ap-southeast-1:123456789012:PipelineNotifications\u0026#34; }, \u0026#34;End\u0026#34;: true } 3. Cost Breakdown S3: ~$0.023/GB/month (Standard storage, ap-southeast-1). Lambda: ~$0.20 per 1M requests + $0.0000167/GB-s. Step Functions: ~$0.025 per 1000 state transitions. EventBridge: ~$1.00 per 1M events. DynamoDB: ~$1.25 per 1M write requests (On-demand mode). SNS: ~$0.50 per 1M notifications. Use AWS Cost Explorer (search in AWS Console) to monitor daily/monthly usage. 4. Cost Optimization Use S3 Intelligent-Tiering to reduce storage cost automatically. Reduce Lambda memory if not needed (256 MB is often sufficient). Use DynamoDB Provisioned Capacity instead of On-demand mode if traffic is predictable. "
},
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/13-scalabilitytesting/",
	"title": "Scalability Testing",
	"tags": [],
	"description": "",
	"content": "Step 11: Scalability Testing 1. Quickly Create 100 CSV Files Create file python: create_csv.py\nimport csv import os import random def create_csv_files(num_files=100, folder_name=\u0026#34;csv_files\u0026#34;): # In ƒë∆∞·ªùng d·∫´n hi·ªán t·∫°i current_path = os.getcwd() print(f\u0026#34;ƒêang ch·∫°y t·ª´: {current_path}\u0026#34;) # T·∫°o ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß full_path = os.path.join(current_path, folder_name) print(f\u0026#34;S·∫Ω t·∫°o file t·∫°i: {full_path}\u0026#34;) # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥ if not os.path.exists(folder_name): os.makedirs(folder_name) print(f\u0026#34;ƒê√£ t·∫°o th∆∞ m·ª•c: {folder_name}\u0026#34;) else: print(f\u0026#34;Th∆∞ m·ª•c {folder_name} ƒë√£ t·ªìn t·∫°i\u0026#34;) # Danh s√°ch t√™n ng·∫´u nhi√™n names = [ \u0026#34;John\u0026#34;, \u0026#34;Jane\u0026#34;, \u0026#34;Alice\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;Charlie\u0026#34;, \u0026#34;Diana\u0026#34;, \u0026#34;Eve\u0026#34;, \u0026#34;Frank\u0026#34;, \u0026#34;Grace\u0026#34;, \u0026#34;Henry\u0026#34;, \u0026#34;Ivy\u0026#34;, \u0026#34;Jack\u0026#34;, \u0026#34;Kate\u0026#34;, \u0026#34;Leo\u0026#34;, \u0026#34;Mia\u0026#34;, \u0026#34;Nick\u0026#34;, \u0026#34;Olivia\u0026#34;, \u0026#34;Paul\u0026#34;, \u0026#34;Quinn\u0026#34;, \u0026#34;Rose\u0026#34;, \u0026#34;Sam\u0026#34;, \u0026#34;Tina\u0026#34;, \u0026#34;Uma\u0026#34;, \u0026#34;Victor\u0026#34;, \u0026#34;Wendy\u0026#34;, \u0026#34;Xavier\u0026#34;, \u0026#34;Yara\u0026#34;, \u0026#34;Zoe\u0026#34;, \u0026#34;Anna\u0026#34;, \u0026#34;Ben\u0026#34;, \u0026#34;Chloe\u0026#34;, \u0026#34;David\u0026#34; ] print(f\u0026#34;B·∫Øt ƒë·∫ßu t·∫°o {num_files} file CSV...\u0026#34;) for i in range(2, num_files + 2): filename = f\u0026#34;{folder_name}/test{i}.csv\u0026#34; with open(filename, \u0026#39;w\u0026#39;, newline=\u0026#39;\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as csvfile: writer = csv.writer(csvfile) # Header writer.writerow([\u0026#39;id\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;amount\u0026#39;]) # T·∫°o d·ªØ li·ªáu ng·∫´u nhi√™n (3-4 d√≤ng m·ªói file) rows = random.randint(3, 4) for j in range(1, rows + 1): row = [ j, random.choice(names), random.randint(50, 9999) # Amount t·ª´ 50 ƒë·∫øn 9999 ] writer.writerow(row) # In progress m·ªói 10 file if i % 10 == 0: print(f\u0026#34;ƒê√£ t·∫°o {i-1} file...\u0026#34;) print(f\u0026#34;‚úÖ HO√ÄN TH√ÄNH! ƒê√£ t·∫°o {num_files} file CSV trong th∆∞ m·ª•c \u0026#39;{folder_name}\u0026#39;\u0026#34;) print(f\u0026#34;üìÅ ƒê∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß: {full_path}\u0026#34;) print(f\u0026#34;üìÑ File t·ª´ test2.csv ƒë·∫øn test{num_files + 1}.csv\u0026#34;) # Li·ªát k√™ 5 file ƒë·∫ßu ti√™n ƒë·ªÉ ki·ªÉm tra print(\u0026#34;\\nüîç Ki·ªÉm tra m·ªôt s·ªë file ƒë√£ t·∫°o:\u0026#34;) for i in range(2, 7): # test2.csv ƒë·∫øn test6.csv filepath = os.path.join(folder_name, f\u0026#34;test{i}.csv\u0026#34;) if os.path.exists(filepath): print(f\u0026#34; ‚úì {filepath}\u0026#34;) else: print(f\u0026#34; ‚úó {filepath} - KH√îNG T√åM TH·∫§Y\u0026#34;) # Ch·∫°y h√†m if __name__ == \u0026#34;__main__\u0026#34;: create_csv_files(100) Nh·∫•n Run ƒë·ªÉ ch·∫°y ch∆∞∆°ng tr√¨nh 2. Load Testing Prepare 100 sample CSV files. Use AWS CLI to upload all files in bulk: aws s3 cp ./test-files/ s3://data-processing-input-123456789012/ --recursive Or upload manually via the S3 Console: Navigate to the data-processing-input-123456789012 bucket Click Upload ‚Üí Select all CSV files ‚Üí Click Upload 3. Performance Monitoring A. Check Number of Executions (in Step Functions) Go to AWS Step Functions Console Select the state machine: DataProcessingWorkflow Open the Executions tab You should see one execution per uploaded file üìå If you upload 100 files ‚Üí you should see around 100 executions\nNote:\nIf the number is lower than expected: EventBridge rule might not be triggering correctly Or Lambda trigger might be failing silently B. Monitor Performance Using CloudWatch 1. View Lambda Metrics: Open CloudWatch Console Navigate to the Metrics tab ‚Üí Click All metrics Select: Lambda ‚Üí By Function Name Choose your Lambda functions: ValidateDataFunction ProcessDataFunction AggregateDataFunction StoreResultsFunction Metric Meaning Duration Average execution time per function. If \u0026gt;1000ms ‚Üí consider optimizing Invocations Number of times the function is triggered (should match number of files) Errors Total number of Lambda errors Throttles Indicates resource limitation or concurrency issues C. Optimize Lambda Functions If Pipeline Is Slow Open Lambda Console ‚Üí Select the function to optimize Go to Configuration ‚Üí General configuration Click Edit Increase Memory size (e.g., from 256MB ‚Üí 512MB or 1024MB) Increasing memory will also boost CPU power ‚Üí better performance Click Save üìå Suggested Configuration:\nFor heavy functions (ProcessDataFunction, AggregateDataFunction): 512‚Äì1024MB For light functions (StoreResultsFunction): 256MB 4. Use Distributed Map (for Large-Scale Data) Edit the state machine definition: Replace the Parallel state with a Map state \u0026#34;MapProcess\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Map\u0026#34;, \u0026#34;ItemsPath\u0026#34;: \u0026#34;$.items\u0026#34;, \u0026#34;MaxConcurrency\u0026#34;: 100, \u0026#34;Iterator\u0026#34;: { \u0026#34;StartAt\u0026#34;: \u0026#34;ProcessData\u0026#34;, \u0026#34;States\u0026#34;: { \u0026#34;ProcessData\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:123456789012:function:ProcessDataFunction\u0026#34;, \u0026#34;End\u0026#34;: true } } }, \u0026#34;Next\u0026#34;: \u0026#34;AggregateData\u0026#34; } Requirements: The input to the Map state must be a JSON array You will need to modify ValidateDataFunction to return a valid array as output "
},
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]