[
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Context and Motivation In today‚Äôs data-driven world, the ability to process and analyze data in an efficient, automated, and scalable manner is essential to supporting modern business processes and decision-making.\nThe project ‚ÄúBuilding a Serverless Data Processing Pipeline with AWS Step Functions and Amazon EventBridge‚Äù introduces a serverless data processing solution, leveraging the robust capabilities of Amazon Web Services (AWS) to meet the needs for high performance, flexibility, and cost-efficiency.\nThe pipeline is designed to automatically process CSV files uploaded to Amazon S3. Amazon EventBridge detects the file upload event and triggers a workflow orchestrated by AWS Step Functions, which includes validation, parallel data processing, result aggregation, and final storage.\nProcessing tasks are executed via AWS Lambda functions. Metadata is stored in Amazon DynamoDB, status notifications are sent through Amazon SNS, and the entire system is monitored via Amazon CloudWatch for performance tracking and error detection.\nKey Benefits of Serverless Data Processing with Step Functions and EventBridge By utilizing core AWS services such as Amazon S3, EventBridge, Step Functions, Lambda, DynamoDB, CloudWatch, and SNS, this solution offers several key advantages:\nFully automated workflows:\nEvent-driven triggers initiate and orchestrate the entire pipeline seamlessly.\nElastic scalability:\nAutomatically scales with incoming workload, supporting parallel data processing.\nCost efficiency:\nPay-as-you-go pricing eliminates infrastructure overhead.\nRobust error handling:\nAutomatic retries, error notifications via SNS, and observability through CloudWatch.\nFlexibility and maintainability:\nModular design using Lambda functions allows for easy updates and debugging.\nPowerful monitoring:\nCloudWatch provides detailed logs and real-time alerts.\nInput data validation:\nEnsures only valid and clean data is processed.\nSimplified operations:\nEasy management of resource lifecycle ‚Äî create, update, and tear down.\nReal-world applicability:\nIdeal for ETL pipelines, batch processing, and real-time event-driven data analytics.\n"
},
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/",
	"title": "Serverless Data Processing Pipeline",
	"tags": [],
	"description": "",
	"content": "Building a Serverless Data Processing Pipeline with AWS Step Functions and EventBridge üìå Overview In today‚Äôs technological landscape, the demand for efficient, flexible, and cost-effective data processing is becoming increasingly critical. Organizations require scalable and easily manageable solutions that eliminate the burden of maintaining complex infrastructure.\nServerless technology on the cloud has emerged as an optimal approach, allowing the development of powerful data processing systems without the need to manage physical servers. This reduces operational costs, accelerates deployment, and enables high automation.\nüß© Pipeline Overview This serverless pipeline processes data from CSV files uploaded to Amazon S3. Amazon EventBridge listens to events from S3, filters for .csv files, and triggers AWS Step Functions to orchestrate the processing workflow. AWS Lambda handles tasks such as validation, parallel processing, aggregation, and result storage. Amazon DynamoDB stores metadata, Amazon CloudWatch monitors performance and errors, and Amazon SNS sends email notifications about the pipeline status.\nThe entire solution is deployed in the Singapore region (ap-southeast-1).\nKey Components Amazon S3\nInput Bucket: Stores the input CSV files\ndata-processing-input-\u0026lt;your-account-id\u0026gt; Output Bucket: Stores processed JSON results\ndata-processing-output-\u0026lt;your-account-id\u0026gt; Amazon EventBridge\nFilters ObjectCreated events from S3 for .csv files and triggers Step Functions AWS Step Functions (State Machine: DataProcessingWorkflow)\nValidateData: Validates the uploaded CSV file ParallelProcess: Executes two parallel branches (ProcessData1, ProcessData2) AggregateData: Aggregates results from both parallel branches StoreResults: Stores the final results to S3 and metadata to DynamoDB NotifySuccess: Sends a success notification via SNS ErrorHandler: Sends an error notification via SNS AWS Lambda\nValidateDataFunction: Validates the CSV format ProcessDataFunction: Processes data (used in each parallel branch) AggregateDataFunction: Aggregates intermediate results StoreResultsFunction: Saves final results and metadata Amazon DynamoDB\nProcessingMetadata table stores information such as ExecutionId, Timestamp, and Status Amazon CloudWatch\nLogs events and sets alarms for failures or anomalies Amazon SNS\nPipelineNotifications topic sends email alerts about pipeline status IAM Roles\nLambdaDataProcessingRole: Grants Lambda permission to access S3, DynamoDB, and SNS StepFunctionsDataProcessingRole: Grants Step Functions permission to invoke Lambda and SNS üîÅ Data Flow A user uploads a CSV file (e.g., test.csv) to the S3 Input Bucket\nS3 triggers an ObjectCreated event, which is sent to EventBridge\nEventBridge Rule (S3TriggerRule) filters .csv files and triggers the Step Functions workflow (DataProcessingWorkflow)\nStep Functions orchestrates the following steps:\nValidateData: Calls ValidateDataFunction to validate the CSV file ParallelProcess: Executes two parallel branches, each invoking ProcessDataFunction AggregateData: Calls AggregateDataFunction to merge results StoreResults: Invokes StoreResultsFunction to save outputs to S3 and metadata to DynamoDB NotifySuccess: Sends a success notification via SNS ErrorHandler: Sends an error notification via SNS if any step fails CloudWatch logs execution and triggers alerts in case of errors\nSNS sends email notifications about the final pipeline status (success or failure)\nContent Introduction Preparation Connect to EC2 instance Manage session logs Port Forwarding Clean up resources "
},
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/2-preparation/",
	"title": "Preparation",
	"tags": [],
	"description": "",
	"content": "Preparation Steps 1. Create IAM Components Step 1: Create an IAM User Group Go to AWS Console: https://console.aws.amazon.com/iam Select User groups ‚Üí Create group Group name: devGr Attach permissions: Choose the existing policy: AdministratorAccess Click Create group Step 2: Create an IAM User Go to Users ‚Üí click Add users Enter user name: dev-user Choose Access Types: Programmatic access (for AWS CLI usage) AWS Management Console access (for web console login) Set a password or let AWS auto-generate one Add the user to group: devGr Click Create user Step 3: Create an AWS Access Key In the user list, click on dev-user Navigate to the Security credentials tab Under Access keys, click Create access key Choose usage purpose: Command Line Interface (CLI) Once created, you will receive: Access key ID Secret access key ‚ö†Ô∏è Save it immediately, the Secret access key is shown only once 2. Install and Configure AWS CLI üîπ Install AWS CLI on Windows Open the Run dialog (Windows + R), paste the following command: msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi Verify the CLI Version aws --version Configure AWS CLI aws configure Provide the following information when prompted:\n\u0026ndash; Access Key ID\n\u0026ndash; Secret Access Key\n\u0026ndash; Region (ap-southeast-1)\n\u0026ndash; Output format (json)\n3. Create a Sample CSV File Create a CSV File Using Notepad Press Windows + R, type notepad, and hit Enter\nPaste the following content:\nid,name,amount 1,John,100 2,Jane,200 Go to File \u0026gt; Save As\nFile name: test.csv\nSave as type: All Files (.)\nEncoding: UTF-8\n‚úÖ Make sure the file is not saved as test.csv.txt\n"
},
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/3-creates3/",
	"title": "Create S3 Buckets",
	"tags": [],
	"description": "",
	"content": "‚òÅÔ∏è Step 1: Create S3 Buckets Amazon S3 is used to store input and output data for the serverless data processing pipeline.\n1. Access the S3 Console Go to AWS Console: https://console.aws.amazon.com/s3 In the search bar, type S3 and select Amazon S3 2. Create Input Bucket Click Create bucket\nConfigure the following:\nBucket name: data-processing-input-123456789012\n(replace 123456789012 with your actual AWS Account ID)\nRegion: Select Asia Pacific (Singapore) - ap-southeast-1\nObject Ownership: Choose ACLs disabled\nBlock Public Access: Leave default (block all public access)\nEncryption: Enable\nServer-side encryption with Amazon S3-managed keys (SSE-S3)\nClick Create bucket\n‚ö†Ô∏è If you receive the error \u0026ldquo;Bucket name already exists\u0026rdquo;, add a suffix to make it unique, e.g.:\ndata-processing-input-123456789012-v1\n3. Create Output Bucket Repeat the same steps as above Use the name: data-processing-output-123456789012 4. Verify the Buckets Go to the Buckets list in the S3 console\nConfirm both buckets are created:\ndata-processing-input-123456789012 data-processing-output-123456789012 Optional: Upload test.csv to Input Bucket Open the data-processing-input-123456789012 bucket Click Upload Select the previously created test.csv file Click Upload to confirm "
},
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/4-create-iam-roles/",
	"title": "T·∫°o IAM Roles",
	"tags": [],
	"description": "",
	"content": "Step 2: Create IAM Roles IAM Roles grant permissions to AWS services such as Lambda and Step Functions to access other AWS resources required during the data processing workflow.\n1. Access the IAM Console Go to AWS Console: https://console.aws.amazon.com/iam Search for IAM ‚Üí go to Roles ‚Üí click Create role 2. Create Role for Lambda Trusted entity type: Select AWS service ‚Üí choose Lambda Click Next Attach Permissions: Click Add permissions\nSearch and attach the following policies:\nAWSLambdaBasicExecutionRole (for writing logs to CloudWatch) AmazonS3FullAccess (to access S3 buckets) AmazonDynamoDBFullAccess (to access DynamoDB) AmazonSNSFullAccess (to send notifications via SNS) üîí Security Note: For production environments, consider creating custom policies with fine-grained access to only the necessary resources.\nClick Next Role name: LambdaDataProcessingRole Click Create role 3. Create Role for Step Functions Return to IAM \u0026gt; Roles ‚Üí click Create role Trusted entity type: Select AWS service ‚Üí choose Step Functions Click Next Attach Permissions: Attach the following policies:\nAWSLambdaFullAccess (allows Step Functions to invoke Lambda functions) AmazonS3FullAccess AmazonDynamoDBFullAccess CloudWatchLogsFullAccess AmazonSNSFullAccess Click Next\nRole name: StepFunctionsDataProcessingRole\nClick Create role\n4. Verify Roles Go to IAM \u0026gt; Roles\nConfirm that both roles are created:\nLambdaDataProcessingRole StepFunctionsDataProcessingRole "
},
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/5-createlambda/",
	"title": "Create Lambda Funtion",
	"tags": [],
	"description": "",
	"content": "Step 3: Create Lambda Functions Create 4 Lambda functions to handle each step of the serverless data processing pipeline.\n1. Access the Lambda Console Open AWS Console: https://console.aws.amazon.com/lambda Search for Lambda, go to Functions, and click Create function 2. Create ValidateDataFunction Function name: ValidateDataFunction Runtime: Python 3.9 (or newer) Architecture: x86_64 Permissions: Choose Use an existing role ‚Üí select LambdaDataProcessingRole Click Create function ‚úÖ Code (Tab: Code \u0026gt; Code source) import json import boto3 import csv import io def lambda_handler(event, context): s3 = boto3.client(\u0026#39;s3\u0026#39;) try: bucket = event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] response = s3.get_object(Bucket=bucket, Key=key) data = response[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;) csv_reader = csv.reader(io.StringIO(data)) headers = next(csv_reader, None) if not headers or len(headers) \u0026lt; 2: raise Exception(\u0026#34;Invalid CSV: Missing or insufficient headers\u0026#34;) first_row = next(csv_reader, None) if not first_row: raise Exception(\u0026#34;Invalid CSV: No data rows\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;bucket\u0026#39;: bucket, \u0026#39;key\u0026#39;: key, \u0026#39;valid\u0026#39;: True, \u0026#39;headers\u0026#39;: headers }) } except Exception as e: raise Exception(f\u0026#34;Validation failed: {str(e)}\u0026#34;) Click Deploy Configuration Timeout: 30 seconds Memory: 256 MB Environment variable: INPUT_BUCKET = data-processing-input-123456789012 3. Create ProcessDataFunction Repeat creation, name the function: ProcessDataFunction import json def lambda_handler(event, context): try: data = json.loads(event[\u0026#39;body\u0026#39;]) bucket = data[\u0026#39;bucket\u0026#39;] key = data[\u0026#39;key\u0026#39;] processed_data = { \u0026#39;result\u0026#39;: f\u0026#34;Processed file {key} from {bucket}\u0026#34;, \u0026#39;transformed\u0026#39;: True } return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(processed_data) } except Exception as e: raise Exception(f\u0026#34;Processing failed: {str(e)}\u0026#34;) Click Deploy Configuration Timeout: 30 seconds Memory: 256 MB 4. Create AggregateDataFunction Repeat creation, name the function: AggregateDataFunction import json def lambda_handler(event, context): try: results = [json.loads(item[\u0026#39;body\u0026#39;]) for item in event] aggregated = { \u0026#39;aggregated_result\u0026#39;: [r[\u0026#39;result\u0026#39;] for r in results], \u0026#39;total_branches\u0026#39;: len(results) } return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(aggregated) } except Exception as e: raise Exception(f\u0026#34;Aggregation failed: {str(e)}\u0026#34;) Click Deploy Configuration Timeout: 30 seconds Memory: 256 MB 5. Create StoreResultsFunction Repeat creation, name the function: StoreResultsFunction import json import boto3 import datetime def lambda_handler(event, context): s3 = boto3.client(\u0026#39;s3\u0026#39;) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(\u0026#39;ProcessingMetadata\u0026#39;) try: data = json.loads(event[\u0026#39;body\u0026#39;]) output_bucket = \u0026#39;data-processing-output-123456789012\u0026#39; # Replace with your actual bucket output_key = f\u0026#34;results/processed_{datetime.datetime.now().isoformat()}.json\u0026#34; s3.put_object(Bucket=output_bucket, Key=output_key, Body=json.dumps(data)) table.put_item(Item={ \u0026#39;ExecutionId\u0026#39;: context.aws_request_id, \u0026#39;Timestamp\u0026#39;: datetime.datetime.now().isoformat(), \u0026#39;OutputKey\u0026#39;: output_key, \u0026#39;Status\u0026#39;: \u0026#39;Success\u0026#39; }) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;status\u0026#39;: \u0026#39;Stored successfully\u0026#39;, \u0026#39;output_key\u0026#39;: output_key}) } except Exception as e: raise Exception(f\u0026#34;Storage failed: {str(e)}\u0026#34;) Replace data-processing-output-123456789012 with your actual S3 bucket name Configuration Timeout: 1 minute Memory: 512 MB 6. Test Each Lambda Function In each Lambda function ‚Üí go to the Test tab ‚Üí click Create test event\nUse the following test payload:\n{ \u0026#34;Records\u0026#34;: [ { \u0026#34;s3\u0026#34;: { \u0026#34;bucket\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;data-processing-input-123456789012\u0026#34;}, \u0026#34;object\u0026#34;: {\u0026#34;key\u0026#34;: \u0026#34;test.csv\u0026#34;} } } ] } Click Test\nCheck logs in Monitor \u0026gt; Logs\n"
},
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/6-createdynamodbtable/",
	"title": "Create DynamoDB Table",
	"tags": [],
	"description": "",
	"content": "Step 4: Create DynamoDB Table Amazon DynamoDB is used to store metadata for each pipeline execution.\n1. Access DynamoDB Console Go to AWS Console Search for DynamoDB ‚Üí select Tables ‚Üí click Create table 2. Configure the Table Table name: ProcessingMetadata Partition key: ExecutionId (Type: String) Sort key: (leave empty) Table settings: Select On-demand (Pay-per-request) for automatic scaling Click Create table 3. Verify the Table Go to the Tables section Ensure the ProcessingMetadata table has been created successfully "
},
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/ws_FCJ_HoangNam/en/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]