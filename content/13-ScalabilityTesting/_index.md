---
title : "Scalability Testing"
date : 2025-06-06 
weight : 13 
chapter : false
pre : " <b> 13. </b> "
---

## Step 11: Scalability Testing

## 1. Quickly Create 100 CSV Files

Create file python: `create_csv.py`


```py
import csv
import os
import random

def create_csv_files(num_files=100, folder_name="csv_files"):
    # In Ä‘Æ°á»ng dáº«n hiá»‡n táº¡i
    current_path = os.getcwd()
    print(f"Äang cháº¡y tá»«: {current_path}")
    
    # Táº¡o Ä‘Æ°á»ng dáº«n Ä‘áº§y Ä‘á»§
    full_path = os.path.join(current_path, folder_name)
    print(f"Sáº½ táº¡o file táº¡i: {full_path}")
    
    # Táº¡o thÆ° má»¥c náº¿u chÆ°a cÃ³
    if not os.path.exists(folder_name):
        os.makedirs(folder_name)
        print(f"ÄÃ£ táº¡o thÆ° má»¥c: {folder_name}")
    else:
        print(f"ThÆ° má»¥c {folder_name} Ä‘Ã£ tá»“n táº¡i")
    
    # Danh sÃ¡ch tÃªn ngáº«u nhiÃªn
    names = [
        "John", "Jane", "Alice", "Bob", "Charlie", "Diana", "Eve", "Frank",
        "Grace", "Henry", "Ivy", "Jack", "Kate", "Leo", "Mia", "Nick",
        "Olivia", "Paul", "Quinn", "Rose", "Sam", "Tina", "Uma", "Victor",
        "Wendy", "Xavier", "Yara", "Zoe", "Anna", "Ben", "Chloe", "David"
    ]
    
    print(f"Báº¯t Ä‘áº§u táº¡o {num_files} file CSV...")
    
    for i in range(2, num_files + 2):
        filename = f"{folder_name}/test{i}.csv"
        
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            
            # Header
            writer.writerow(['id', 'name', 'amount'])
            
            # Táº¡o dá»¯ liá»‡u ngáº«u nhiÃªn (3-4 dÃ²ng má»—i file)
            rows = random.randint(3, 4)
            for j in range(1, rows + 1):
                row = [
                    j,
                    random.choice(names),
                    random.randint(50, 9999)  # Amount tá»« 50 Ä‘áº¿n 9999
                ]
                writer.writerow(row)
        
        # In progress má»—i 10 file
        if i % 10 == 0:
            print(f"ÄÃ£ táº¡o {i-1} file...")
    
    print(f"âœ… HOÃ€N THÃ€NH! ÄÃ£ táº¡o {num_files} file CSV trong thÆ° má»¥c '{folder_name}'")
    print(f"ðŸ“ ÄÆ°á»ng dáº«n Ä‘áº§y Ä‘á»§: {full_path}")
    print(f"ðŸ“„ File tá»« test2.csv Ä‘áº¿n test{num_files + 1}.csv")
    
    # Liá»‡t kÃª 5 file Ä‘áº§u tiÃªn Ä‘á»ƒ kiá»ƒm tra
    print("\nðŸ” Kiá»ƒm tra má»™t sá»‘ file Ä‘Ã£ táº¡o:")
    for i in range(2, 7):  # test2.csv Ä‘áº¿n test6.csv
        filepath = os.path.join(folder_name, f"test{i}.csv")
        if os.path.exists(filepath):
            print(f"  âœ“ {filepath}")
        else:
            print(f"  âœ— {filepath} - KHÃ”NG TÃŒM THáº¤Y")

# Cháº¡y hÃ m
if __name__ == "__main__":
    create_csv_files(100)
```
- Nháº¥n Run Ä‘á»ƒ cháº¡y chÆ°Æ¡ng trÃ¬nh

## 2. Load Testing

- Prepare 100 sample CSV files.
- Use AWS CLI to upload all files in bulk:  
  ```bash
  aws s3 cp ./test-files/ s3://data-processing-input-123456789012/ --recursive
  ```
![Connect](/ws_FCJ_HoangNam/images/11.ScalabilityTesting/B11.png)
- Or upload manually via the **S3 Console**:
  - Navigate to the `data-processing-input-123456789012` bucket
  - Click **Upload** â†’ Select all CSV files â†’ Click **Upload**

## 3. Performance Monitoring

### A. Check Number of Executions (in Step Functions)

- Go to **AWS Step Functions Console**
- Select the state machine: `DataProcessingWorkflow`
- Open the **Executions** tab
- You should see one execution per uploaded file

ðŸ“Œ If you upload 100 files â†’ you should see around **100 executions**

**Note:**
- If the number is lower than expected:
  - EventBridge rule might not be triggering correctly
  - Or Lambda trigger might be failing silently

---

### B. Monitor Performance Using CloudWatch

#### 1. View Lambda Metrics:

- Open **CloudWatch Console**
- Navigate to the **Metrics** tab â†’ Click **All metrics**
- Select: **Lambda â†’ By Function Name**
- Choose your Lambda functions:
  - `ValidateDataFunction`
  - `ProcessDataFunction`
  - `AggregateDataFunction`
  - `StoreResultsFunction`

| Metric      | Meaning |
|-------------|---------|
| Duration    | Average execution time per function. If >1000ms â†’ consider optimizing |
| Invocations | Number of times the function is triggered (should match number of files) |
| Errors      | Total number of Lambda errors |
| Throttles   | Indicates resource limitation or concurrency issues |

![Connect](/ws_FCJ_HoangNam/images/11.ScalabilityTesting/B11_3.png)
![Connect](/ws_FCJ_HoangNam/images/11.ScalabilityTesting/B11_4.png)
![Connect](/ws_FCJ_HoangNam/images/11.ScalabilityTesting/B11_5.png)
![Connect](/ws_FCJ_HoangNam/images/11.ScalabilityTesting/B11_6.png)


---

### C. Optimize Lambda Functions If Pipeline Is Slow

- Open **Lambda Console** â†’ Select the function to optimize
- Go to **Configuration â†’ General configuration**
- Click **Edit**
  - Increase **Memory size** (e.g., from 256MB â†’ 512MB or 1024MB)
  - Increasing memory will also boost CPU power â†’ better performance
- Click **Save**

ðŸ“Œ **Suggested Configuration:**
- For heavy functions (`ProcessDataFunction`, `AggregateDataFunction`): **512â€“1024MB**
- For light functions (`StoreResultsFunction`): **256MB**

---

## 4. Use Distributed Map (for Large-Scale Data)

- Edit the state machine definition:
  - Replace the `Parallel` state with a `Map` state
```js
    "MapProcess": {
  "Type": "Map",
  "ItemsPath": "$.items",
  "MaxConcurrency": 100,
  "Iterator": {
    "StartAt": "ProcessData",
    "States": {
      "ProcessData": {
        "Type": "Task",
        "Resource": "arn:aws:lambda:ap-southeast-1:123456789012:function:ProcessDataFunction",
        "End": true
      }
    }
  },
  "Next": "AggregateData"
}

``` 
- Requirements:
  - The input to the `Map` state must be a **JSON array**
  - You will need to modify `ValidateDataFunction` to return a valid array as output

